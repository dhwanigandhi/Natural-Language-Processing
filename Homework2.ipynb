{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "from nltk import *\n",
    "import re\n",
    "from nltk.corpus import treebank\n",
    "from statistics import mean \n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus = PlaintextCorpusReader('.', '.*\\.txt')\n",
    "Text = mycorpus.raw(\"new_file.txt\")\n",
    "Text = Text.replace(\"reviewText:\",\"\")\n",
    "NewText = nltk.sent_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = re.compile(\".*\\?$\")\n",
    "question_sent = []\n",
    "for i in NewText:\n",
    "    if re.match(string,i):\n",
    "        question_sent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_tagged = treebank.tagged_sents()\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(treebank_tagged, backoff=t0)\n",
    "t2 = nltk.BigramTagger(treebank_tagged, backoff=t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokentext = [nltk.word_tokenize(sent) for sent in question_sent]\n",
    "taggedtext = [t2.tag(tokens) for tokens in tokentext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = r\"\"\"JJP: {<RB.*><JJ.*>|<DT><JJ.*>|<JJ.*><NN.*>|<JJ.*><PRP.*>|<W.*><JJ.*>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(rule)\n",
    "adj_question_sent = []\n",
    "for i in taggedtext:\n",
    "    x = chunkParser.parse(i)\n",
    "    for subtree in x.subtrees(filter = lambda t: t.label() == 'JJP'):\n",
    "        if i in adj_question_sent:\n",
    "            continue\n",
    "        else:\n",
    "            adj_question_sent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "c = ''\n",
    "for i in range(len(adj_question_sent)):\n",
    "    a = adj_question_sent[i]\n",
    "    for j in range(len(a)):\n",
    "        b = a[j][0]+' '\n",
    "        c = c+b\n",
    "    sentence = c     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7528"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2648"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adj_question_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.12273413897281"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for i in range(len(adj_question_sent)):\n",
    "    length.append(len(adj_question_sent[i]))\n",
    "\n",
    "mean(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens = nltk.word_tokenize(sentence) \n",
    "Words = [w.lower() for w in Tokens]\n",
    "RevisedWords = [w for w in Words if w.isalpha()]\n",
    "Stopwords = nltk.corpus.stopwords.words('english')\n",
    "StoppedRevisedWords = [w for w in RevisedWords if w not in Stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('would', 259)\n",
      "('like', 249)\n",
      "('one', 199)\n",
      "('size', 197)\n",
      "('good', 194)\n",
      "('maybe', 180)\n",
      "('could', 163)\n",
      "('get', 159)\n",
      "('wear', 146)\n",
      "('really', 138)\n",
      "('pair', 132)\n",
      "('fit', 128)\n",
      "('watch', 122)\n",
      "('little', 122)\n",
      "('price', 120)\n",
      "('know', 120)\n",
      "('much', 119)\n",
      "('want', 115)\n",
      "('right', 113)\n",
      "('say', 112)\n",
      "('great', 109)\n",
      "('shoe', 104)\n",
      "('comfortable', 102)\n",
      "('shoes', 101)\n",
      "('look', 100)\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "Fdist = FreqDist(StoppedRevisedWords) \n",
    "Top25 = Fdist.most_common(25)  \n",
    "for pair in Top25: \n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isalpha(w):\n",
    "    if w.isalpha():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(Words)\n",
    "finder.apply_word_filter(isalpha)\n",
    "finder.apply_word_filter(lambda w: w in Stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('look', 'like'), 0.0002612671456564337)\n",
      "(('looks', 'like'), 0.0002286087524493795)\n",
      "(('many', 'times'), 0.0002286087524493795)\n",
      "(('great', 'price'), 0.0002122795558458524)\n",
      "(('another', 'pair'), 0.00019595035924232528)\n",
      "(('good', 'quality'), 0.00017962116263879816)\n",
      "(('good', 'thing'), 0.00016329196603527107)\n",
      "(('high', 'quality'), 0.00016329196603527107)\n",
      "(('looks', 'great'), 0.00016329196603527107)\n",
      "(('feel', 'like'), 0.00014696276943174395)\n",
      "(('first', 'pair'), 0.00013063357282821686)\n",
      "(('first', 'time'), 0.00013063357282821686)\n",
      "(('larger', 'size'), 0.00013063357282821686)\n",
      "(('smaller', 'size'), 0.00013063357282821686)\n",
      "(('wrong', 'size'), 0.00013063357282821686)\n",
      "(('every', 'time'), 0.00011430437622468974)\n",
      "(('gon', 'na'), 0.00011430437622468974)\n",
      "(('good', 'price'), 0.00011430437622468974)\n",
      "(('high', 'end'), 0.00011430437622468974)\n",
      "(('light', 'weight'), 0.00011430437622468974)\n",
      "(('long', 'time'), 0.00011430437622468974)\n",
      "(('shoe', 'size'), 0.00011430437622468974)\n",
      "(('two', 'pairs'), 0.00011430437622468974)\n",
      "(('well', 'made'), 0.00011430437622468974)\n",
      "(('another', 'one'), 9.797517962116264e-05)\n"
     ]
    }
   ],
   "source": [
    "for BigramScore in scored[:25]:\n",
    "    print(BigramScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finderp = BigramCollocationFinder.from_words(Words)\n",
    "finderp.apply_word_filter(isalpha)\n",
    "finderp.apply_word_filter(lambda w: w in Stopwords)\n",
    "finderp.apply_freq_filter(5)\n",
    "scoredp = finderp.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('gon', 'na'), 12.73226166094484)\n",
      "(('midway', 'briefs'), 12.442755043749854)\n",
      "(('bathing', 'suit'), 12.17972063791606)\n",
      "(('customer', 'service'), 12.17972063791606)\n",
      "(('flip', 'flops'), 12.165221068220944)\n",
      "(('arch', 'support'), 10.41675983521691)\n",
      "(('free', 'shipping'), 9.99529606677863)\n",
      "(('cold', 'water'), 9.82537106533632)\n",
      "(('high', 'end'), 8.978222553419693)\n",
      "(('light', 'weight'), 8.864051533500382)\n",
      "(('ever', 'seen'), 8.857792543028697)\n",
      "(('many', 'times'), 8.46280098595161)\n",
      "(('two', 'pairs'), 8.458059173824545)\n",
      "(('real', 'deal'), 8.166123034515337)\n",
      "(('left', 'foot'), 8.143297229666187)\n",
      "(('best', 'part'), 8.088405471170113)\n",
      "(('anyone', 'else'), 7.540242888651909)\n",
      "(('running', 'shoe'), 7.523675039133421)\n",
      "(('every', 'time'), 7.379745246224054)\n",
      "(('different', 'sizes'), 7.2493416893851705)\n",
      "(('high', 'quality'), 7.222706562881704)\n",
      "(('second', 'pair'), 7.1948275303062665)\n",
      "(('low', 'price'), 7.12082694886249)\n",
      "(('another', 'pair'), 6.888166192072214)\n",
      "(('full', 'size'), 6.695172342209618)\n"
     ]
    }
   ],
   "source": [
    "for PmiScore in scoredp[:25]:\n",
    "    print(PmiScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = re.compile(\".*!$\")\n",
    "exclamation_sent = []\n",
    "for i in NewText:\n",
    "    if re.match(string,i):\n",
    "        exclamation_sent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokentext1 = [nltk.word_tokenize(sent) for sent in exclamation_sent]\n",
    "taggedtext1 = [t2.tag(tokens) for tokens in tokentext1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1 = r\"\"\"VP: {^<V.*>|^<DT><V.*>}\"\"\"\n",
    "chunkParser1 = nltk.RegexpParser(rule1)\n",
    "imperative_sent = []\n",
    "for i in taggedtext1:\n",
    "    x1 = chunkParser1.parse(i)\n",
    "    for subtree in x1.subtrees(filter = lambda t: t.label() == 'VP'):\n",
    "        if i in imperative_sent:\n",
    "            continue\n",
    "        else:\n",
    "            imperative_sent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2 = r\"\"\"ADJ_VP: {<RB.*><JJ.*>|<JJ.*><CC>|<DT><JJ.*>|<JJ.*><PRP.*>}\"\"\"\n",
    "chunkParser2 = nltk.RegexpParser(rule2)\n",
    "adj_imperative_sent = []\n",
    "for i in imperative_sent:\n",
    "    x2 = chunkParser2.parse(i)\n",
    "    for subtree in x2.subtrees(filter = lambda t: t.label() == 'ADJ_VP'):\n",
    "        if i in adj_imperative_sent:\n",
    "            continue\n",
    "        else:\n",
    "            adj_imperative_sent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "z = ''\n",
    "for i in range(len(adj_imperative_sent)):\n",
    "    x = adj_imperative_sent[i]\n",
    "    for j in range(len(x)):\n",
    "        y = x[j][0]+' '\n",
    "        z = z+y\n",
    "    sent = z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5288"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imperative_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1982"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adj_imperative_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.80827447023209"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length1 = []\n",
    "for i in range(len(adj_imperative_sent)):\n",
    "    length1.append(len(adj_imperative_sent[i]))\n",
    "    \n",
    "mean(length1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens1 = nltk.word_tokenize(sent) \n",
    "Words1 = [w.lower() for w in Tokens1]\n",
    "RevisedWords1 = [w for w in Words1 if w.isalpha()]\n",
    "StoppedRevisedWords1 = [w for w in RevisedWords1 if w not in Stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('great', 415)\n",
      "('comfortable', 288)\n",
      "('cute', 183)\n",
      "('nice', 165)\n",
      "('ever', 157)\n",
      "('really', 152)\n",
      "('good', 148)\n",
      "('best', 136)\n",
      "('one', 120)\n",
      "('like', 120)\n",
      "('shoes', 119)\n",
      "('pair', 118)\n",
      "('price', 118)\n",
      "('beautiful', 110)\n",
      "('wear', 106)\n",
      "('buy', 101)\n",
      "('fit', 97)\n",
      "('size', 96)\n",
      "('love', 92)\n",
      "('quality', 88)\n",
      "('first', 85)\n",
      "('well', 83)\n",
      "('look', 78)\n",
      "('time', 74)\n",
      "('bought', 72)\n"
     ]
    }
   ],
   "source": [
    "Fdist1 = FreqDist(StoppedRevisedWords1) \n",
    "top25 = Fdist1.most_common(25)  \n",
    "for pair in top25: \n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures1 = nltk.collocations.BigramAssocMeasures()\n",
    "finder1 = BigramCollocationFinder.from_words(Words1)\n",
    "finder1.apply_word_filter(isalpha)\n",
    "finder1.apply_word_filter(lambda w: w in Stopwords)\n",
    "scored1 = finder1.score_ngrams(bigram_measures1.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('make', 'sure'), 0.0008314781535820615)\n",
      "(('really', 'nice'), 0.0008314781535820615)\n",
      "(('ever', 'owned'), 0.0007510125258160555)\n",
      "(('ever', 'worn'), 0.0005632593943620417)\n",
      "(('well', 'made'), 0.0005632593943620417)\n",
      "(('great', 'buy'), 0.0005364375184400397)\n",
      "(('great', 'price'), 0.0005364375184400397)\n",
      "(('really', 'cute'), 0.0005364375184400397)\n",
      "(('first', 'pair'), 0.00048279376659603575)\n",
      "(('good', 'quality'), 0.00048279376659603575)\n",
      "(('comfortable', 'shoes'), 0.0004559718906740337)\n",
      "(('flip', 'flops'), 0.0004559718906740337)\n",
      "(('first', 'time'), 0.00042915001475203173)\n",
      "(('super', 'cute'), 0.00042915001475203173)\n",
      "(('second', 'pair'), 0.00037550626290802777)\n",
      "(('comfortable', 'shoe'), 0.0003218625110640238)\n",
      "(('great', 'purchase'), 0.0003218625110640238)\n",
      "(('high', 'quality'), 0.0003218625110640238)\n",
      "(('really', 'great'), 0.0003218625110640238)\n",
      "(('feel', 'like'), 0.0002950406351420218)\n",
      "(('great', 'quality'), 0.0002950406351420218)\n",
      "(('look', 'great'), 0.0002950406351420218)\n",
      "(('looks', 'great'), 0.0002950406351420218)\n",
      "(('extremely', 'comfortable'), 0.00026821875922001983)\n",
      "(('great', 'deal'), 0.00026821875922001983)\n"
     ]
    }
   ],
   "source": [
    "for BigramScore in scored1[:25]:\n",
    "    print(BigramScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder1p = BigramCollocationFinder.from_words(Words1)\n",
    "finder1p.apply_word_filter(isalpha)\n",
    "finder1p.apply_word_filter(lambda w: w in Stopwords)\n",
    "finder1p.apply_freq_filter(3)\n",
    "scored1p = finder1p.score_ngrams(bigram_measures1.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('stainless', 'steel'), 13.601267830853399)\n",
      "(('reasonably', 'priced'), 12.601267830853399)\n",
      "(('swim', 'suit'), 12.186230331574553)\n",
      "(('previous', 'reviews'), 10.864302236687191)\n",
      "(('flip', 'flops'), 10.777838145937725)\n",
      "(('jewelry', 'cleaner'), 10.77683939543685)\n",
      "(('nursing', 'bras'), 10.156482988180501)\n",
      "(('highly', 'recommend'), 10.02523845490225)\n",
      "(('arch', 'support'), 10.016305330132244)\n",
      "(('new', 'balance'), 9.900828112712304)\n",
      "(('several', 'times'), 9.864302236687191)\n",
      "(('rain', 'boots'), 9.424679099130074)\n",
      "(('must', 'say'), 9.201337223964764)\n",
      "(('year', 'old'), 9.063833700214827)\n",
      "(('light', 'weight'), 9.006321241559618)\n"
     ]
    }
   ],
   "source": [
    "for PmiScore in scored1p[:15]:\n",
    "    print(PmiScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
